Introduction: Nowadays, the autonomous driving is gaining much popularity. For example, it creates a potential to provide mobility options for individuals who are unable to drive so as to promote inclusivity within society. It can also improve traffic efficiency and reduce congestion so that less fuel consumption and more environmentally friendly transportation can be made possible. However, the safety of the autonomous vehicles should also be concerned. One crucial aspect of autonomous car development is behavioral training, which involves training the car to respond accordingly and appropriately to various driving circumstances instead of jeopardizing pedestrians. 
Research objective: 
The objective is to leverage the power of deep learning algorithms to train the car to accurately perceive, interpret and react to different driving situations. The research significance is that it can contribute to the advancement of autonomous car technology by proposing a deep learning-based approach for behavioral training, thereby paving the way for safer and more efficient transportation systems.
Methodology: 
The simulator(provided by Udacity) provides input data(driving_log.csv) and images from different angles of the car. So it acts as a server and transmits these images and data logs to the Python client.The client is the machine learning model built using deep neural networks. These models are developed on Keras. Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets. Once the model is trained, it provides steering angles to drive to the server. So the inputs can prevent autonomous car from keeping off the track.
Solution Implementation:
Firstly, I import all the libraries needed for training process. It uses Tensorflow backend and Keras at frontend.Then I extract the driving log csv file from the data directory naming as “udacity-self-driving-car-behavioral-cloning” which also includes many different images available to me. I set the 7 columns: center, left, right, steering, throttle, reverse, speed. Because odd numbers tend to show center distribution and I want to visualize the quantity of steering samples , I set the number bins to 27 and keep samples per bin at 400 and utilize the histogram function to present intuitively.
Then I load the images from the dataset directory into the array to append the steering data from center, left and right images. Next I split the path of image and store arrays meanwhile so that I get the 3045 training samples and 762 valid samples.  
DNN architecture:
The network is based on the NVIDIA model. I define the model structure by defining the model objects. The Nvidia model modified uses 24 filters in the layer along with a kernel of size 5,5. Then sub sampling is introduced. The function reflects to stride length of the kernel as it processes through an image. Horizontal and vertical movement are set to 2 pixels at a time. As this is the first layer, I have to define input shape of the model (66,200,3) and the last function is an activation function that is “elu”. 
The second layer is quite similar, with 36 filters and kernel size (5,5). Because there are three more layers in the CNN(48 filters,64 filters (3,3) kernel,64 filters (3,3) kernel), I remove the sub sampling from 4th and 5th layer. Then I add a flatten layer to convert the output layer from the convolutional layer before to one-dimensional array. 
The last layer is a dense layer containing a single output node which will output the predicted steering angle for the self driving car. Next I compile the architecture and use low learning rate to increase accuracy. Now I can have the training process by importing training data x_train and y_train that require more epochs to be effective.
